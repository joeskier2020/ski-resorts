{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d611041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP Processing\n",
    "# Processing by number of stars in the review\n",
    "\n",
    "#  The first step is to read in the dataset.  \n",
    "#  This data set contains 'ski resort reviews'\n",
    "#  And the source is https://www.kaggle.com/datasets/fredkellner/onthesnow-ski-area-reviews\n",
    "\n",
    "import pandas as pd\n",
    "reviews = pd.read_csv ('OnTheSnow_SkiAreaReviews.csv')\n",
    "#  give the dataset friendlier column names.\n",
    "reviews.columns = ['placeholder','state','ski_area','reviewer_name','review_date',\n",
    "                        'review_stars','review_text']\n",
    "\n",
    "#  break up the dataset into 5 subsets by the number of stars in the review\n",
    "reviews_1star = reviews[reviews['review_stars']==1]\n",
    "reviews_2star = reviews[reviews['review_stars']==2]\n",
    "reviews_3star = reviews[reviews['review_stars']==3]\n",
    "reviews_4star = reviews[reviews['review_stars']==4]\n",
    "reviews_5star = reviews[reviews['review_stars']==5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c89a7e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "#  This defines a function to pre-process the text by \n",
    "#  removing any numbers and punctuation.  It tokenizes the sentences into words. \n",
    "#  It also removes common stop words in the english language.  \n",
    "#  Finally it lemmatizes the words to have only roots of the words not endings. \n",
    "\n",
    "def get_clean_review(df):\n",
    "    clean_review = []\n",
    "    for index, row in df.iterrows():\n",
    "        review_str = str(row['review_text'])\n",
    "        # split the string into a list of words\n",
    "        tokens = word_tokenize(review_str)\n",
    "        lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "        # removes anything not alpha characters such as punctuation and numbers\n",
    "        alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "        # removes the stop words from the text\n",
    "        no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
    "\n",
    "        # Lemmatize all tokens into a new list: lemmatized\n",
    "        lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "        lemmatized = no_stops\n",
    "        \n",
    "        # make a list of lists\n",
    "        for w in lemmatized:\n",
    "            clean_review.append(w)\n",
    "        \n",
    "    return clean_review\n",
    "\n",
    "\n",
    "#  The function returns a list of words broken down by the number of stars\n",
    "#  There are 5 documents which will later be used in Gensim to do TFIDF analysis\n",
    "\n",
    "review_for_dictionary1 = get_clean_review(reviews_1star)\n",
    "review_for_dictionary2 = get_clean_review(reviews_2star)\n",
    "review_for_dictionary3 = get_clean_review(reviews_3star)\n",
    "review_for_dictionary4 = get_clean_review(reviews_4star)\n",
    "review_for_dictionary5 = get_clean_review(reviews_5star)\n",
    "\n",
    "#  Gensim requires the 5 documents to be in list forms.  Each document is a list \n",
    "#  of cleaned words.  The below line creates a list of lists.  \n",
    "\n",
    "review_for_dictionary=[review_for_dictionary1,review_for_dictionary2,review_for_dictionary3,review_for_dictionary4,review_for_dictionary5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d98fb08f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a8c8d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "\n",
    "# print(clean_review[0:9])\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "# bow = Counter(clean_review)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "# print(bow.most_common(40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aa28132f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the length of the reviews:\n",
      "94185\n",
      "77809\n",
      "123956\n",
      "266096\n",
      "431507\n"
     ]
    }
   ],
   "source": [
    "# frequencies from gensim\n",
    "# pip install --upgrade gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "counter = 1\n",
    "print(\"Here is the number of words in reviews of the reviews:\")\n",
    "for r in review_for_dictionary:\n",
    "    print(\"There are\", len(r), \"in the\",counter,\"star reviews\")\n",
    "    counter += 1 \n",
    "    \n",
    "dictionary = Dictionary(review_for_dictionary)\n",
    "#print(dictionary)\n",
    "#print(clean_review[0:10])\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "#dictionary = Dictionary(clean_review)\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(r) for r in review_for_dictionary]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "503abb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the lengths of the documents in the corpus\n",
      "7674\n",
      "7103\n",
      "9046\n",
      "12920\n",
      "16017\n",
      "These are the words in the ratings with 1  star\n",
      "ski 1316\n",
      "lift 1290\n",
      "mountain 1156\n",
      "snow 955\n",
      "great 918\n",
      "These are the words in the ratings with 2  star\n",
      "ski 1131\n",
      "mountain 947\n",
      "lift 923\n",
      "great 824\n",
      "snow 771\n",
      "These are the words in the ratings with 3  star\n",
      "ski 1790\n",
      "mountain 1648\n",
      "lift 1548\n",
      "great 1335\n",
      "day 1185\n",
      "These are the words in the ratings with 4  star\n",
      "ski 3668\n",
      "great 3450\n",
      "mountain 3406\n",
      "lift 3129\n",
      "snow 2812\n",
      "These are the words in the ratings with 5  star\n",
      "ski 6852\n",
      "great 6243\n",
      "mountain 5690\n",
      "lift 4718\n",
      "snow 4375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "count =1 \n",
    "\n",
    "print(\"Here are the lengths of the documents in the corpus\")\n",
    "for doc in corpus:\n",
    "    print(len(doc))\n",
    "\n",
    "for doc in  corpus:\n",
    "    print(\"These are the words in the ratings with\", count, \" star\")\n",
    "    count +=1\n",
    "    # Sort the doc for frequency: bow_doc\n",
    "    bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "    # Print the top 5 words of the document alongside the count\n",
    "    for word_id, word_count in bow_doc[:5]:\n",
    "        print(dictionary.get(word_id), word_count)\n",
    "\n",
    "    # Create the defaultdict: total_word_count\n",
    "    total_word_count = defaultdict(int)\n",
    "    for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "        total_word_count[word_id] += word_count\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "97efe1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "714dc4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b4b65761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "85c4c5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  Star Reviews key words are:\n",
      "holliday 0.2\n",
      "illness 0.105\n",
      "sledders 0.105\n",
      "jp 0.1\n",
      "mc 0.1\n",
      "der 0.09\n",
      "es 0.088\n",
      "thunderstruck 0.088\n",
      "wandered 0.07\n",
      "woodloch 0.07\n",
      "\n",
      " 2  Star Reviews key words are:\n",
      "mc 0.11\n",
      "coperate 0.086\n",
      "initiated 0.086\n",
      "kelleys 0.086\n",
      "majic 0.086\n",
      "meh 0.086\n",
      "sustainable 0.086\n",
      "millions 0.073\n",
      "blady 0.064\n",
      "chump 0.064\n",
      "\n",
      " 3  Star Reviews key words are:\n",
      "song 0.095\n",
      "division 0.094\n",
      "filed 0.094\n",
      "sanford 0.079\n",
      "tuff 0.079\n",
      "activated 0.063\n",
      "cables 0.063\n",
      "clyde 0.063\n",
      "enuff 0.063\n",
      "garlic 0.063\n",
      "\n",
      " 4  Star Reviews key words are:\n",
      "brule 0.096\n",
      "clair 0.071\n",
      "invoice 0.071\n",
      "bleecker 0.062\n",
      "comparisons 0.062\n",
      "presentation 0.062\n",
      "kaatskill 0.061\n",
      "honeycomb 0.059\n",
      "afton 0.055\n",
      "hmsr 0.053\n",
      "\n",
      " 5  Star Reviews key words are:\n",
      "brule 0.331\n",
      "josh 0.087\n",
      "nub 0.078\n",
      "monarch 0.07\n",
      "saloon 0.065\n",
      "homestead 0.056\n",
      "peruvian 0.056\n",
      "coolest 0.053\n",
      "disappoints 0.05\n",
      "blackjack 0.048\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "counter = 1\n",
    "for doc in corpus:\n",
    "    tfidf_weights = tfidf[doc]\n",
    "    \n",
    "    print('\\n',counter,\" Star Reviews key words are:\")\n",
    "    counter +=1\n",
    "    \n",
    "    # Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "    sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "    # Print the top 5 weighted words\n",
    "    for term_id, weight in sorted_tfidf_weights[:10]:\n",
    "        print(dictionary.get(term_id), round(weight,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e9798f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.tfidfmodel.TfidfModel"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4f9c60fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jackson-hole\n",
      "lutsen-mountains\n",
      "mountain-creek-resort\n",
      "boyne-mountain-resort\n",
      "crystal-mountain\n",
      "shanty-creek\n",
      "nubs-nob-ski-area\n",
      "loon-mountain\n",
      "cannonsburg\n",
      "alpine-valley-resort\n",
      "boyne-highlands\n",
      "caberfae-peaks-ski-golf-resort\n",
      "devils-head\n",
      "mount-brighton\n",
      "mount-holly\n",
      "spring-mountain-ski-area\n"
     ]
    }
   ],
   "source": [
    "# find a ski_center with a particular word.\n",
    "import re \n",
    "\n",
    "search_word = \"nub\"\n",
    "search_word = search_word.lower()\n",
    "\n",
    "temp = ''\n",
    "for i in range(len(reviews)) :\n",
    "    r,sc = reviews.loc[i, \"review_text\"], reviews.loc[i, \"ski_area\"]\n",
    "    if not r != r:\n",
    "        r = r.lower()\n",
    "        if re.search(search_word,r):\n",
    "            if sc != temp: \n",
    "                print(sc)\n",
    "                temp = sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a0d397b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = reviews.loc[0,'review_text']\n",
    "r != r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3165066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_named_entity_list(df):\n",
    "    ne = []\n",
    "    for index, row in df.iterrows():\n",
    "        review_str = str(row['review_text'])\n",
    "        # Tokenize the article into sentences: sentences\n",
    "        sentences = sent_tokenize(review_str)\n",
    "\n",
    "        # Tokenize each sentence into words: token_sentences\n",
    "        token_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "        # Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "        pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "        # Create the named entity chunks: chunked_sentences\n",
    "        chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "        \n",
    "        # Test for stems of the tree with 'NE' tags\n",
    "        for sent in chunked_sentences:\n",
    "            for chunk in sent:\n",
    "                if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "                    ne.append(chunk)\n",
    "    return ne\n",
    "\n",
    "ne_chunks = []             \n",
    "ne_chunks.append(get_named_entity_list(reviews_1star))\n",
    "ne_chunks.append(get_named_entity_list(reviews_2star))\n",
    "ne_chunks.append(get_named_entity_list(reviews_3star))\n",
    "ne_chunks.append(get_named_entity_list(reviews_4star))\n",
    "ne_chunks.append(get_named_entity_list(reviews_5star))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fae121a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (2669513967.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [87]\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(type(chunked_sentences)\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "print(type(chunked_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1becf141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[Tree('NE', [('Squaw', 'NNP')]), Tree('NE', [('Squaw', 'NNP')]), Tree('NE', [('Sugar', 'NNP'), ('Bowl', 'NNP')]), Tree('NE', [('Sugar', 'NNP'), ('Bowl', 'NNP')]), Tree('NE', [('Squaw', 'NNP')]), Tree('NE', [('Squaw', 'NNP')])]\n",
      "[Tree('NE', [('Plenty', 'NN')]), Tree('NE', [('Skip', 'NNP'), ('Tahoe', 'NNP')]), Tree('NE', [('Utah', 'NNP')]), Tree('NE', [('Colorado', 'NNP')]), Tree('NE', [('Canadian', 'JJ'), ('Rockies', 'NNPS')]), Tree('NE', [('Tahoe', 'NNP')])]\n",
      "[Tree('NE', [('Due', 'NNP')]), Tree('NE', [('Super', 'NNP')]), Tree('NE', [('WTF', 'NNP')]), Tree('NE', [('Large', 'JJ')]), Tree('NE', [('Squaw', 'NNP')]), Tree('NE', [('Solitude', 'NNP')])]\n",
      "[Tree('NE', [('SV', 'NNP')]), Tree('NE', [('Squaw', 'NNP'), ('Valley', 'NNP')]), Tree('NE', [('Visibility', 'NN')]), Tree('NE', [('Great', 'NNP'), ('Expert', 'NNP'), ('Mountain', 'NNP')]), Tree('NE', [('Alpine', 'NNP'), ('Meadows', 'NNP')]), Tree('NE', [('Squaw', 'NNP'), ('Squaw', 'NNP'), ('Valley', 'NNP')])]\n"
     ]
    }
   ],
   "source": [
    "print(type(ne_chunks))\n",
    "\n",
    "print(ne_chunks[0][0:6])\n",
    "print(ne_chunks[1][0:6])\n",
    "print(ne_chunks[2][0:6])\n",
    "print(ne_chunks[3][0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6cd0d0b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtfidfmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfModel\n\u001b[0;32m----> 3\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mTfidfModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mne_chunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Calculate the tfidf weights of doc: tfidf_weights\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#counter = 1\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#for doc in corpus:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m tfidf_weights \u001b[38;5;241m=\u001b[39m tfidf[ne_chunks[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/tfidfmodel.py:384\u001b[0m, in \u001b[0;36mTfidfModel.__init__\u001b[0;34m(self, corpus, id2word, dictionary, wlocal, wglobal, normalize, smartirs, pivot, slope)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2word \u001b[38;5;241m=\u001b[39m dictionary\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m corpus:\n\u001b[0;32m--> 384\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;66;03m# NOTE: everything is left uninitialized; presumably the model will\u001b[39;00m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;66;03m# be initialized in some other way\u001b[39;00m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/tfidfmodel.py:449\u001b[0m, in \u001b[0;36mTfidfModel.initialize\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m    447\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROGRESS: processing document #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, docno)\n\u001b[1;32m    448\u001b[0m     numnnz \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(bow)\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m termid, _ \u001b[38;5;129;01min\u001b[39;00m bow:\n\u001b[1;32m    450\u001b[0m         dfs[termid] \u001b[38;5;241m=\u001b[39m dfs\u001b[38;5;241m.\u001b[39mget(termid, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# keep some stats about the training corpus\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "tfidf = TfidfModel(ne_chunks)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "#counter = 1\n",
    "#for doc in corpus:\n",
    "tfidf_weights = tfidf[ne_chunks[0]]\n",
    "    \n",
    "#    print('\\n',counter,\" Star Reviews key words are:\")\n",
    "#    counter +=1\n",
    "    \n",
    "    # Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "    # Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:10]:\n",
    "    print(dictionary.get(term_id), round(weight,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed42b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "  \n",
    "# Input the file \n",
    "txt1 = []\n",
    "with open('C:\\\\Users\\\\DELL\\\\Desktop\\\\MachineLearning1.txt') as file:\n",
    "    txt1 = file.readlines()\n",
    "  \n",
    "# Preprocessing\n",
    "def remove_string_special_characters(s):\n",
    "      \n",
    "    # removes special characters with ' '\n",
    "    stripped = re.sub('[^a-zA-z\\s]', '', s)\n",
    "    stripped = re.sub('_', '', stripped)\n",
    "      \n",
    "    # Change any white space to one space\n",
    "    stripped = re.sub('\\s+', ' ', stripped)\n",
    "      \n",
    "    # Remove start and end white spaces\n",
    "    stripped = stripped.strip()\n",
    "    if stripped != '':\n",
    "            return stripped.lower()\n",
    "          \n",
    "# Stopword removal \n",
    "stop_words = set(stopwords.words('english'))\n",
    "your_list = ['skills', 'ability', 'job', 'description']\n",
    "for i, line in enumerate(txt1):\n",
    "    txt1[i] = ' '.join([x for \n",
    "        x in nltk.word_tokenize(line) if \n",
    "        ( x not in stop_words ) and ( x not in your_list )])\n",
    "      \n",
    "# Getting trigrams \n",
    "vectorizer = CountVectorizer(ngram_range = (3,3))\n",
    "X1 = vectorizer.fit_transform(txt1) \n",
    "features = (vectorizer.get_feature_names())\n",
    "print(\"\\n\\nFeatures : \\n\", features)\n",
    "print(\"\\n\\nX1 : \\n\", X1.toarray())\n",
    "  \n",
    "# Applying TFIDF\n",
    "vectorizer = TfidfVectorizer(ngram_range = (3,3))\n",
    "X2 = vectorizer.fit_transform(txt1)\n",
    "scores = (X2.toarray())\n",
    "print(\"\\n\\nScores : \\n\", scores)\n",
    "  \n",
    "# Getting top ranking features\n",
    "sums = X2.sum(axis = 0)\n",
    "data1 = []\n",
    "for col, term in enumerate(features):\n",
    "    data1.append( (term, sums[0,col] ))\n",
    "ranking = pd.DataFrame(data1, columns = ['term','rank'])\n",
    "words = (ranking.sort_values('rank', ascending = False))\n",
    "print (\"\\n\\nWords head : \\n\", words.head(7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
